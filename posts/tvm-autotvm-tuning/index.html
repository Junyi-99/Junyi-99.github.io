<!doctype html><html lang=en-us data-theme><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>TVM AutoTVM 多 GPU 加速 Tuning - Junyi&rsquo;s Lab</title><meta name=description content="场景
双 NVIDIA 1080Ti, Linux 服务器
需求
AutoTVM 的 Tuning 可以让两张 CUDA 显卡同时运行模型 （AMD 的没试过）"><link rel=icon type=image/x-icon href=https://www.junyi.dev/favicon.ico><link rel=apple-touch-icon-precomposed href=https://www.junyi.dev/favicon.png><link rel=stylesheet href=https://www.junyi.dev/css/style.min.5aca35955b4a4a2b3987b33115c6840f82dd4fb76a33d9d54ec059a3c019dd8c.css integrity="sha256-Wso1lVtKSis5h7MxFcaED4LdT7dqM9nVTsBZo8AZ3Yw="><script src=https://www.junyi.dev/js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><meta property="og:title" content="TVM AutoTVM 多 GPU 加速 Tuning"><meta property="og:description" content="场景
双 NVIDIA 1080Ti, Linux 服务器
需求
AutoTVM 的 Tuning 可以让两张 CUDA 显卡同时运行模型 （AMD 的没试过）"><meta property="og:type" content="article"><meta property="og:url" content="https://www.junyi.dev/posts/tvm-autotvm-tuning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-02-27T23:32:44+08:00"><meta property="article:modified_time" content="2021-02-27T23:32:44+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="TVM AutoTVM 多 GPU 加速 Tuning"><meta name=twitter:description content="场景
双 NVIDIA 1080Ti, Linux 服务器
需求
AutoTVM 的 Tuning 可以让两张 CUDA 显卡同时运行模型 （AMD 的没试过）"></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=/>Junyi's Lab</a></h1><ul class=social-icons><li><a href=https://github.com/Junyi-99 title=Github rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a></li></ul></div><nav><a href=https://www.junyi.dev/ title>Home</a>
<a href=https://www.junyi.dev/tags/ title>Tags</a>
<a href=https://www.junyi.dev/posts/ title>Archive</a>
<a href=https://www.junyi.dev/about/ title>About</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">TVM AutoTVM 多 GPU 加速 Tuning</h1></header></div><div class="content e-content"><p><strong>场景</strong>
双 NVIDIA 1080Ti, Linux 服务器</p><p><strong>需求</strong>
AutoTVM 的 Tuning 可以让两张 CUDA 显卡同时运行模型 （AMD 的没试过）</p><p><strong>过程</strong></p><blockquote><p>当前场景为 tuner 和 runner 都跑在同一台机器上的情况
如果需要一台跑 tuner，一台跑 runner，请配置好 tracker 和 server 的 IP
tracker 和 server 都可以放在有显卡的机器里</p></blockquote><ol><li>首先查看一下 GPU 的 id，你可以通过这条命令来查看：</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>nvidia-smi -L
</span></span></code></pre></div><p>比如我这里输出的是 0 和 1：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>(</span>base<span style=color:#f92672>)</span> admin@deeplearning:~$ nvidia-smi -L
</span></span><span style=display:flex><span>GPU 0: GeForce GTX <span style=color:#ae81ff>1080</span> Ti <span style=color:#f92672>(</span>UUID: GPU-a4602aba-35cb-97cd-ef5a-f7d12aabdc88<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>GPU 1: GeForce GTX <span style=color:#ae81ff>1080</span> Ti <span style=color:#f92672>(</span>UUID: GPU-92deed64-3b37-f0a4-1095-40f0f596d64b<span style=color:#f92672>)</span>
</span></span></code></pre></div><ol start=2><li>用 screen 创建 1 个 rpc_tracker</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>screen -S tvm_tune_tracker
</span></span><span style=display:flex><span>python -m tvm.exec.rpc_tracker --host<span style=color:#f92672>=</span>0.0.0.0 --port<span style=color:#f92672>=</span><span style=color:#ae81ff>9190</span>
</span></span></code></pre></div><p>输入完之后，如果 tracker 运行成功，那就按下 CTRL+A 然后按下 CTRL+D，detach 当前的 screen</p><ol start=3><li>用 screen 创建 2 个 rpc_server（因为我有 2 个 GPU）</li></ol><p>还记得第一步查看的 GPU id 吗？下面的 `CUDA_VISIBLE_DEVICES`` 就要设置成刚才看的 GPU 的 id</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>screen -S tvm_tune_server0
</span></span><span style=display:flex><span>export CUDA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>python -m tvm.exec.rpc_server --tracker<span style=color:#f92672>=</span>127.0.0.1:9190 --key<span style=color:#f92672>=</span>1080ti
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 输入完之后，如果 server0 运行成功，那就按下 CTRL+A 然后按下 CTRL+D，detach 当前的 screen</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>screen -S tvm_tune_server1
</span></span><span style=display:flex><span>export CUDA_VISIBLE_DEVICES<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>python -m tvm.exec.rpc_server --tracker<span style=color:#f92672>=</span>127.0.0.1:9190 --key<span style=color:#f92672>=</span>1080ti
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 输入完之后，如果 server1 运行成功，那就按下 CTRL+A 然后按下 CTRL+D，detach 当前的 screen</span>
</span></span></code></pre></div><p>注意，上面的 <code>export</code> 是重点！
如果在 shell 里直接不写 <code>export</code> ，直接写 <code>CUDA_VISIBLE_DEVICES=0</code> 是无效的！！！！
尽管不写 <code>export</code> 时，<code>echo $CUDA_VISIBLE_DEVICES</code> 也会输出 1，但是实际上并没有设置环境变量。
原因是 autotvm 会通过 python 的 <code>os.environ['CUDA_VISIBLE_DEVICES']</code> 的值来决定使用哪一个 id 的 GPU。</p><h1 id=检测效果>检测效果：
<span><a href=#%e6%a3%80%e6%b5%8b%e6%95%88%e6%9e%9c><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>按照官方教程修改 device 为 1080ti，ip 地址和端口记得也要按照自己的实际情况进行修改，然后运行 tune_relay_cuda.py
Extract tasks 之后，你应该可以通过 <code>nvidia-smi</code> 看到 GPU 的利用情况：</p><p><code>watch -n 0.5 nvidia-smi</code> （意思是每隔 0.5s 运行 nvidia-smi 命令一次）</p><img src=result.png alt=drawing width=800><p>可以看到两张显卡都已经利用起来了。</p><p>– END –</p><h1 id=2023年8月23日-补充>2023年8月23日 补充
<span><a href=#2023%e5%b9%b48%e6%9c%8823%e6%97%a5-%e8%a1%a5%e5%85%85><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h1><p>看到自己之前写的文章，感觉好幼稚啊哈哈哈哈</p></div><div class=post-info><div class="post-date dt-published"><a class=u-url href=/posts/tvm-autotvm-tuning/><time datetime=2021-02-27>2021-02-27</time></a></div><a class="post-hidden-url u-url" href=https://www.junyi.dev/posts/tvm-autotvm-tuning/>https://www.junyi.dev/posts/tvm-autotvm-tuning/</a>
<a href=https://www.junyi.dev/ class="p-name p-author post-hidden-author h-card" rel=me>Junyi Hou</a><div class=post-taxonomies><ul class=post-tags><li><a href=https://www.junyi.dev/tags/tvm/>#TVM</a></li><li><a href=https://www.junyi.dev/tags/autotvm/>#AutoTVM</a></li></ul></div></div></article><h3 class=read-next-title>Read next</h3><ul class=read-next-posts><li><a href=/posts/tvm-autotvm-raspberrypy/>TVM AutoTVM 树莓派 4B 调优</a></li><li><a href=/posts/tvm-compile-on-macos/>TVM 在 macOS 上的编译</a></li></ul></main><footer class=common-footer><div class=common-footer-bottom><div class=copyright><p>© Junyi Hou, 2023<br>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.<br></p></div></div><p class="h-card vcard"><a href=https://www.junyi.dev/ class="p-name u-url url fn" rel=me>Junyi Hou</a>
/
<a class="p-email u-email email" rel=me href=mailto:hhh@u.nus.edu>hhh@u.nus.edu</a></p></footer></div></body></html>